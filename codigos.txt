##DIA1
setwd("datasets")
dados<-read.csv("prehistoric_dog.csv", header = TRUE, sep=";")

####
boxplot(dados[-1])

####
cor(dados[-1])

####
stars(dados[-1], col.stars=seq(1,to = length(dados)),
 labels=c("modern_dog","golden_jackal","chinese_wolf",
 "indian_wolf","cuon","dingo","prehistoric_dog"))
####
install.packages('TeachingDemos')
require(TeachingDemos)
faces(dados[-1], scale=TRUE, labels=c("modern_dog","golden_jackal",
  "chinese_wolf","indian_wolf","cuon","dingo","prehistoric_dog"))

####
Standardization
pad1<-function(x)(x-mean(x))/sd(x)
a<- apply(dados[-1],2,pad1)
scale(dados[-1])

minmax<-function(x)(x-min(x))/(max(x)-min(x))
b<- apply(dados[-1],2,minmax)

#### HC
standard_variables = scale(dados[-1]) 
d<- dist(standard_variables,"euclidean")
hc<- hclust(d)
dc <- as.dendrogram(hc)
plot(dc)

#### K-means
set.seed(123)
km<- kmeans(d, 3, nstart=10)
clusk <- km$cluster
o <- order(clusk)
stars(dados[o,], col.stars=clusk[o]+1)

#### Principal components

p <- prcomp(standard_variables)
p
summary(p)
biplot(p, scale=0)
###Dia2
########## Advertising
install.packages("hnp")
require("hnp")

df<- read.csv("advertising.csv", header = TRUE)
head(df)
str(df)
summary(df)
pairs(df)
cor(df)

model1 = lm(Sales~., data=df)
summary(model1)
confint(model1)
hnp(model1, print.on=TRUE)

model2 = lm(Sales~TV+Radio, df)
summary(model2)
hnp(model2, print.on=TRUE)

model3 = lm(Sales~Radio+TV+I(TV^2), df)
summary(model3)
hnp(model3, print.on=TRUE)

pred<-predict(model3)

plot(df$Sales, pred)

df$pred<-pred
head(df)

df$e2<-(df$pred - df$Sales)^2
head(df)

MSE <- sum(df$e2)/ (dim(df)[1])
MSE

######### Diaphorina
dados<-read.csv("diaphorina.csv")
head(dados)
unique(dados$oil)
str(dados)

dados$oil<-factor(dados$oil)

install.packages(MASS)
require(MASS)
require(hnp)

#Poisson regression
model1 <- glm(y ~ oil-1, family=poisson, dados)
summary(model1)

#Quasi-binomial
model2 <- glm(y ~ oil-1, family=quasipoisson, dados)
summary(model2)
summary(model2)$dispersion

#Negative Binomial
model3 <- glm.nb(y ~ oil-1, dados)
summary(model3)
summary(model3)$theta

#Half normal-plot
par(mfrow=c(1,3),cex=1.4, cex.main=0.9, pty='s')
hnp(model1,pch=4, main="(a) Poisson",
    xlab="Half-normal scores", ylab="Deviance residuals",print.on=TRUE)
hnp(model2,pch=4, main="(b) Quasi-Poisson",
    xlab="Half-normal scores", ylab='',print.on=TRUE)#"Deviance residuals"
hnp(model3,pch=4, main="(c) Negative binomial",
    xlab="Half-normal scores", ylab='',print.on=TRUE)#, ylab="Deviance residuals")


# function to get confidence intervals for a negative binomial model
confint.negbin <- function(model, nrep, glres, ...) {
  phi <- model$theta
  X <- model.matrix(model)
  mi <- fitted(model)
  eta <- predict(model)
  th <- model$theta
  W <- diag(model$weights)
  Cov <- X%*%solve(t(X)%*%W%*%X)%*%t(X)
  Cv_eta <- diag(Cov)
  p2 <- (sqrt(Cv_eta)*qt(.975, glres))
  ICinf <- eta - p2
  ICsup <- eta + p2
  ICinf <- exp(ICinf)
  ICsup <- exp(ICsup)
  ntrat <- ncol(X)
  r <- data.frame(ICinf,mi,ICsup)
  r <- data.frame(r, 'a'=1:nrep)
  r <- r[order(r$a),]
  r <- r[1:ntrat,-4]
  rownames(r) <- 1:ntrat
  IC <- as.numeric(unlist(r))
  E <- gl(ntrat, 3)
  stripchart(IC~E, vert=T, pch='', ...)
  points(r$mi, pch=16)
  arrows(1:ntrat,r$ICinf,1:ntrat,r$ICsup,angle=90,code=3,length=0.1)
  return(r)
}

# confidence interval plot
confint.negbin(model3, nrep=10, glres=63, xaxt="n", ylab="Egg counts")
axis(1, 1:7 )

############# Loan

require(hnp)
require(ggplot2)

df<- read.csv("bank.csv", header = TRUE)
str(df)

fit1<-glm(loan~. ,family="binomial",data=df)

summary(fit1)
hnp(fit1, print.on=TRUE)

grid <- expand.grid(
  balance = seq(0, 2700, length = 100),
  income = seq(700, 75000, length = 100)
)


grid$prob <- predict(fit1, grid, type="response")
grid$pred <- factor(ifelse(grid$prob < .5, "No", "Yes"))

ggplot(aes(x=balance,y=income, color=pred), data=grid)+
  geom_point(size=.3)

#Stepwise
step(fit1)

#Likelihood ratio test
fit2<-glm(loan~balance ,family="binomial",data=df)
anova(fit2,fit1, test = "Chisq")


##Exemplo regression tree
require(ggplot2)
require(tree)
require(randomForest)

iris<- read.csv("iris_flower.csv")
iris1 = iris[c(1:11,51:59),]


tree <- tree(Species ~ Sepal.Length+Sepal.Width, data=iris1)
summary(tree)


par(mfrow=c(1,2))
plot(Sepal.Length~Sepal.Width, data=iris1,col=Species, pch=20)
partition.tree(tree, ordvars=c("Sepal.Width","Sepal.Length"), add=TRUE)
plot(tree)
text(tree, cex=.9, pretty=0)


## All data
tree <- tree(Species ~ Sepal.Length + Sepal.Width, data = iris)
summary(tree)

grid <- expand.grid(
Sepal.Length = seq(min(iris$Sepal.Length), max(iris$Sepal.Length),
 length = 100),
Sepal.Width = seq(min(iris$Sepal.Width), max(iris$Sepal.Width),
length = 100)
)


plot(tree)
text(tree, cex = .7)

pred <- predict(tree, newdata = grid, type = "class")

ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species), data = iris) +
  geom_point() +
  geom_point(aes(color = pred), data = grid, alpha = .4)

pred2<-predict(tree, type="class")
table(iris$Species, pred2)


### Random forest
bag <- randomForest(Species ~ Sepal.Length+Sepal.Width, data=iris)
pred <- predict(bag, grid)

ggplot(aes(x=Sepal.Length,y=Sepal.Width, color=Species), data=iris)+
  geom_point()+
  geom_point(aes(color=pred),data=grid, alpha=.4)


### Artificial neural network

require("neuralnet")

dados<-read.csv("dividend.csv")
head(dados)
str(dados)

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

maxmindf <- as.data.frame(lapply(dados, normalize))
head(maxmindf)

# Training and Test Data
trainset <- maxmindf[1:160, ]
testset <- maxmindf[161:200, ]

nn <- neuralnet(dividend ~ fcfps + earnings_growth + de + mcap + current_ratio,
                data=trainset, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)

#Test the resulting output
temp_test <- subset(testset, 
    select = c("fcfps","earnings_growth", "de", "mcap", "current_ratio"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testset$dividend, prediction = nn.results$net.result)

roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)

table(roundedresultsdf$actual,roundedresultsdf$prediction)

accuracy = 100* (17+20)/ 40
accuracy

#Chalenge

rm(list=ls(all=TRUE))
dados <- read.csv("california_housing.csv", header=TRUE, sep=",")
head(dados)
str(dados)

#Pacotes
require("tree")
require("ggplot2")
require("randomForest")
require("neuralnet")

#Regressao linear
lr<-lm(MedianHouseValue~., data=dados)
# Regression tree
rtree <- tree(MedianHouseValue~., data=dados)
# Random forests
rforests <- randomForest(MedianHouseValue~., data=dados, ntree=100)


MSElr<- sum((predict(lr) - dados$MedianHouseValue)^2)/dim(dados)[1]
MSErt<- sum((predict(rtree) - dados$MedianHouseValue)^2)/dim(dados)[1]
MSErf<- sum((predict(rforests) - dados$MedianHouseValue)^2)/dim(dados)[1]

MSE<- c(MSElr,MSErt,MSErf)
models<-c("linear regression","regression tree","random forest")
result = data.frame(MSE,models)
result[order(result$MSE),]
varImpPlot(rforests)

dados$predRF<-predict(rforests)
head(dados)

ggplot(data=dados, aes(x=Longitude, y=Latitude, color=predRF))+ geom_point()
ggplot(data=dados, aes(x=Longitude, y=Latitude, color=MedianIncome))+ geom_point()
